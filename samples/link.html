<html>
  <head><title>links</title></head>
  <body>
    <h1>Link(s)</h1>
    <ul>
      <li><a href="https://hackernoon.com/designing-api-contracts-for-legacy-system-modernization">designing api contracts for legacy system modernization</a></li>
      <li><a href="https://www.databricks.com/feed">databricks feed</a></li>
      <li><a href="https://roundup.getdbt.com/feed">getdbt.com feed</a></li>
    </ul>

    <h1>articles</h1>
    <a href="https://gradientflow.substack.com/p/data-engineering-for-machine-users">Data Engineering in 2026: What Changes?</a>
    <p>In the 2026 agent-native era, data engineering focuses on AI agents as the primary "machine users," prioritizing automation, reliability, and safety amid agents driving over 80% of new database creations and surging prompt volumes. Key shifts include modular PARK (PyTorch, AI Models, Ray, and Kubernetes) stacks, multimodal lakehouses, context stores for business logic, confidence-gated execution for safety, and ephemeral databases.</p>

    <a href="https://blog.gopenai.com/from-local-to-global-a-deep-dive-into-graphrag-1c50e2fc9e65">From Local to Global: A Deep Dive into GraphRAG</a>
    <p>GraphRAG enhances conventional RAG by building a hierarchical knowledge graph from raw documents, enabling both granular local retrieval and global corpus-level reasoning for LLMs. GraphRAG synthesizes high-level insights and reduces hallucinations by using token-overlapping chunking, entity resolution, weighted relationships, and recursive community detection. The dual Local/Global retrieval modes resolve prior trade-offs between precision and corpus-wide understanding at the cost of pre-computation.</p>

    <a href="https://ragaboutit.com/the-adaptive-retrieval-advantage-why-query-aware-rag-is-replacing-one-size-fits-all-retrieval/">The Adaptive Retrieval Advantage: Why Query-Aware RAG Is Replacing One-Size-Fits-All Retrieval</a>
    <p>Lightweight query classification slashes RAG costs and latency while boosting precision for complex enterprise queries.</p>

    <a href="https://www.philschmid.de/mcp-cli">Introducing MCP CLI: A way to call MCP Servers Efficiently</a>
    <p>2026-01-09</p>
    <p>MCP-CLI is a lightweight, open-source command-line tool that enables efficient, dynamic interaction with Model Context Protocol (MCP) servers. By supporting just-in-time tool discovery and execution instead of statically loading all tool definitions, it drastically reduces token consumption (up to 99% savings), making it ideal for AI coding agents like Gemini CLI or Claude Code.</p>

    <a href="https://www.pinecone.io/learn/rag-access-control/">RAG with Access Control</a>
    <p>2026-01-08</p>
    <p>Relationship-Based Access Control (ReBAC) models data access as a graph of relationships between users and resources, providing flexibility for dynamic and context-rich applications. SpiceDB, an open-source Zanzibar-based ReBAC implementation, can be integrated with vector databases like Pinecone to secure RAG pipelines with fine-grained policies. OpenAI uses SpiceDB to enforce access policies over 37 billion documents across 5 million ChatGPT Connector users, effectively preventing information leakage when serving domain-specific knowledge.</p>

    <a href="https://www.dataengineeringweekly.com/p/a-critique-of-iceberg-rest-catalog">A Critique of Iceberg REST Catalog: A Classic Case of Why Semantic Spec Fails</a>
    <p>2026-01-09</p>
    <p>Apache Iceberg's REST Catalog specification ensures semantic interoperability, enabling diverse engines such as Trino, Spark, and Flink to interact seamlessly via a universal API. However, the standard omits operational guarantees (no defined latency, throughput, or synchronization SLAs), leading to unpredictable performance, high retry amplification, and systemic instability as table and catalog counts scale. This lack of operational constraints shifts the burden to clients and operators, making systems fragile and hard to maintain, highlighting the need for explicit behavioral contracts and conformance testing to ensure reliability at enterprise scale.</p>

    <a href="https://thenewstack.io/openeverest-a-tool-to-manage-multiple-databases-on-kubernetes/">OpenEverest, a Tool To Manage Multiple Databases on Kubernetes</a>
    <p>2026-01-09</p>
    <p>Percona has donated OpenEverest, a unified Kubernetes-native database management tool that supports PostgreSQL, MySQL, and MongoDB, to the CNCF under Apache 2.0 open source licensing. OpenEverest enables vendor-agnostic provisioning, high availability, disaster recovery, and autoscaling through standardized CRDs and RESTful APIs, simplifying database ops without requiring database-specific expertise.</p>

    <a href="https://ai.gopubby.com/autorag-the-end-of-guesswork-in-retrieval-augmented-generation-cc9ac0ad578c"> AutoRAG: The End of Guesswork in Retrieval-Augmented Generation</a>
    <p>2026-01-12</p>
    <p>AutoRAG is a system that automatically tests and tunes different RAG pipeline setups to find what works best for each task. Instead of hand-tuning chunking, retrieval, reranking, and prompts, you define the evaluation metric and let it optimize the pipeline end-to-end. </p>

    <a href="https://www.ssp.sh/blog/diary-of-a-data-engineer/"> A Diary of a Data Engineer</a>
    <p>2026-01-13</p>
    <p> The core essence and challenges of data engineering remain unchanged despite decades of tool evolution from early ETL/SSIS in the 2000s, through Hadoop/big data hype, to modern stacks like dbt, Iceberg, and AI agents in 2026. Key insights include the eternal loop ("The tools change. The loop doesn't."), the lost art of proper data modeling, treating schema issues as people problems, and focusing on fundamentals over trends.</p>

    <a href="https://medium.com/@zeta-decoded/zetas-lakehouse-journey-a-composable-scalable-and-federated-architecture-df0ab5f19c3a">Zeta’s Lakehouse Journey: A Composable, Scalable, and Federated Architecture </a>
    <p>2026-01-13</p>
    <p>Zeta built a composable, scalable, and federated Lakehouse architecture with Apache Iceberg on S3Table/AWS Glue Catalog and a multi-account federated setup to unify heterogeneous data from acquisitions, enable secure cross-team access without duplication, support diverse compute engines (Spark, Snowflake, ClickHouse, and Trino) for AI/marketing workloads, and ensure governance, schema evolution, and vendor independence. </p>

    <a href="https://github.com/matsonj/mviz">mviz (GitHub Repo)</a>
    <p> </p>
    <p>mviz is a lightweight tool that lets you turn small JSON specs into polished HTML or PDF reports through Claude, so you can explore data, iterate quickly, and share results without building dashboards or infrastructure. It focuses on fast, AI-native analysis workflows with minimal boilerplate and high-quality static outputs that are easy to export and reuse.</p>

    <a href="https://towardsdatascience.com/you-probably-dont-need-a-vector-database-for-your-rag-yet/">You Probably Don't Need a Vector Database for Your RAG — Yet</a>
    <p>2026-01-20</p>
    <p>For small-to-medium RAG pipelines, NumPy and scikit-learn can deliver millisecond-level in-memory vector search on millions of records, eliminating the overhead and complexity of dedicated vector databases like Pinecone or Weaviate. Using pure matrix multiplication and tree-based search (KD/Ball-Tree), typical workloads up to ~1.5GB of embeddings (e.g., 1M vectors × 384-dim) perform efficiently without network serialization or CRUD demands. Transition to vector databases only when persistence, high-frequency updates, metadata filtering, or RAM limits are required.</p>

    <a href="https://www.astronomer.io/blog/building-data-pipelines-like-assembly-lines/">Building Data Pipelines Like Assembly Lines</a>
    <p>2026-01-19</p>
    <p>A small data engineering team stopped building one-off Airflow pipelines and instead created a declarative factory where every dataset follows the same write, test, and publish pattern. By encoding testing, documentation, dependency management, and safety into reusable components, they can ship new pipelines in hours instead of days and avoid bad data reaching production.</p>

    <a href="https://medium.com/@sanjeebmeister/apache-spark-performance-tuning-on-amazon-emr-a-complete-guide-part2-3a0deda7d0c7">Apache Spark Performance Tuning on Amazon EMR</a>
    <p>2026-01-19</p>
    <p>Optimal Spark performance on EMR demands strategic tuning, rather than additional compute, to prevent wasted costs and degradation from shuffle overhead, GC pressure, and network contention. Key optimizations include executor sizing, memory management, partition and shuffle tuning, caching, column/prune elimination, predicate pushdown, handling small files, and leveraging Z-ordering for “haystack queries”. Benchmarks show a 10x difference in 1TB S3 read times based solely on data organization and configuration.</p>
    
    <a href="https://dropbox.tech/machine-learning/vp-josh-clemm-knowledge-graphs-mcp-and-dspy-dash">Engineering VP Josh Clemm on How We Use Knowledge Graphs, MCP, and DSPy in Dash </a>
    <p>2026-01-28</p>
    <p> By giving Dash access to proprietary work content, it unifies search, Q&A, and agentic tasks across Dropbox files and third-party apps. Dash ingests data via custom connectors, generates multimodal embeddings and knowledge graphs for entity relationships, and uses hybrid retrieval (BM25 + dense vectors) for fast retrieval. It optimizes MCP tool calling and tunes 30+ prompts (including LLM-as-judge) for better relevance, fewer disagreements, and easier model iteration.</p>
    
    <a href="https://loglevelinfo.substack.com/p/how-i-structure-my-data-pipelines-a20">How I Structure My Data Pipelines: The Silver Layer </a>
    <p>2026-01-29</p>
    <p>The Silver layer combines Medallion (Bronze-Silver-Gold) with Kimball dimensional modeling, serving as the core by organizing data into business-domain schemas with facts (granular events) and dimensions (attributes with surrogate keys), using intermediates for reusable transformations, and RLS/CLM access controls. This design ensures predictability, schema evolution, isolation of business logic in Silver, and composability. </p>
    
    <a href="https://netflixtechblog.medium.com/data-bridge-how-netflix-simplifies-data-movement-36d10d91c313"> Data Bridge: How Netflix simplifies data movement</a>
    <p> </p>
    <p>Netflix's Data Bridge unifies and abstracts batch data movement across more than three dozen source-destination pairs, eliminating fragmentation from bespoke tools. As a programmable control plane, it orchestrates ~300,000 jobs per week via a no-code/low-code interface, intent-based API, and YAML configs, centralizing metadata, governance, and job management. The platform's pluggable architecture streamlines connector contributions and enables seamless transitions to new data movement implementations. </p>
    
    <a href="https://openai.com/index/inside-our-in-house-data-agent/">Inside OpenAI's in-house data agent </a>
    <p>2026-01-29</p>
    <p>OpenAI built a bespoke internal AI data agent powered by GPT-5 that lets employees ask natural-language questions and get accurate, contextual data insights end to end, from table discovery to analysis and reporting. It combines code-aware data context, institutional knowledge, memory, and continuous evaluation to deliver fast, reliable analytics at OpenAI's scale. </p>
    
    <a href=""> </a>
    <p> </p>
    <p> </p>
  </body>
</html>
